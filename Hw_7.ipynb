{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Module(object):\n",
        "    \"\"\"\n",
        "    Basically, you can think of a module as of a something (black box)\n",
        "    which can process `input` data and produce `ouput` data.\n",
        "    This is like applying a function which is called `forward`:\n",
        "\n",
        "        output = module.forward(input)\n",
        "\n",
        "    The module should be able to perform a backward pass: to differentiate the `forward` function.\n",
        "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
        "    The latter implies there is a gradient from previous step of a chain rule.\n",
        "\n",
        "        gradInput = module.backward(input, gradOutput)\n",
        "    \"\"\"\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "        self.training = True\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes an input object, and computes the corresponding output of the module.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input)\n",
        "\n",
        "    def backward(self,input, gradOutput):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the module, with respect to the given input.\n",
        "\n",
        "        This includes\n",
        "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
        "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
        "        \"\"\"\n",
        "        self.updateGradInput(input, gradOutput)\n",
        "        self.accGradParameters(input, gradOutput)\n",
        "        return self.gradInput\n",
        "\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        \"\"\"\n",
        "        Computes the output using the current parameter set of the class and input.\n",
        "        This function returns the result which is stored in the `output` field.\n",
        "\n",
        "        Make sure to both store the data in `output` field and return it.\n",
        "        \"\"\"\n",
        "\n",
        "        # The easiest case:\n",
        "\n",
        "        # self.output = input\n",
        "        # return self.output\n",
        "\n",
        "        pass\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own input.\n",
        "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
        "\n",
        "        The shape of `gradInput` is always the same as the shape of `input`.\n",
        "\n",
        "        Make sure to both store the gradients in `gradInput` field and return it.\n",
        "        \"\"\"\n",
        "\n",
        "        # The easiest case:\n",
        "\n",
        "        # self.gradInput = gradOutput\n",
        "        # return self.gradInput\n",
        "\n",
        "        pass\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own parameters.\n",
        "        No need to override if module has no parameters (e.g. ReLU).\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        \"\"\"\n",
        "        Zeroes `gradParams` variable if the module has params.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def getParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with its parameters.\n",
        "        If the module does not have parameters return empty list.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with gradients with respect to its parameters.\n",
        "        If the module does not have parameters return empty list.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Sets training mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Sets evaluation mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want\n",
        "        to have readable description.\n",
        "        \"\"\"\n",
        "        return \"Module\""
      ],
      "metadata": {
        "id": "PiZwSgGVLUMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sequential(Modules):\n",
        "  def __init__(self):\n",
        "    super(Sequential, self).__init__()\n",
        "    self.modules = []\n",
        "\n",
        "  def add(self, module):\n",
        "    return self.modules.append(module)\n",
        "\n",
        "  def updateOutput(self, input):\n",
        "    current_input = input\n",
        "    for module in self.modules:\n",
        "      current_input = module.forward(current_input)\n",
        "    self.output = current_input\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, input, gradOutput):\n",
        "    current_grad = gradOutput\n",
        "    current_input = input\n",
        "    for i in range(len(modules) -1, -1, -1):\n",
        "      if i ==0:\n",
        "        current_input = input\n",
        "      else:\n",
        "        current_input = self.modules[i-1].output\n",
        "      current_grad = module.backward(current_input, current_grad)\n",
        "    self.gradInput = current_grad\n",
        "    return self.grad_input\n",
        "\n",
        "  def zeroGradParameters(self):\n",
        "        for module in self.modules:\n",
        "            module.zeroGradParameters()\n",
        "\n",
        "  def getParameters(self):\n",
        "      \"\"\"\n",
        "      Should gather all parameters in a list.\n",
        "      \"\"\"\n",
        "      return [x.getParameters() for x in self.modules]\n",
        "\n",
        "  def getGradParameters(self):\n",
        "      \"\"\"\n",
        "      Should gather all gradients w.r.t parameters in a list.\n",
        "      \"\"\"\n",
        "      return [x.getGradParameters() for x in self.modules]\n",
        "\n",
        "  def __repr__(self):\n",
        "      string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
        "      return string\n",
        "\n",
        "  def __getitem__(self,x):\n",
        "      return self.modules.__getitem__(x)\n",
        "\n",
        "  def train(self):\n",
        "      \"\"\"\n",
        "      Propagates training parameter through all modules\n",
        "      \"\"\"\n",
        "      self.training = True\n",
        "      for module in self.modules:\n",
        "          module.train()\n",
        "\n",
        "  def evaluate(self):\n",
        "      \"\"\"\n",
        "      Propagates training parameter through all modules\n",
        "      \"\"\"\n",
        "      self.training = False\n",
        "      for module in self.modules:\n",
        "          module.evaluate()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CBvSh5lNYW3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(Module):\n",
        "  def __init__(self, n_in, n_out):\n",
        "    super(Linear, self).__init__()\n",
        "    stdv = 1./np.sqrt(n_in)\n",
        "    self.W = np.random.uniform(-stdv, stdv, size = (n_in, n_out))\n",
        "    self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
        "\n",
        "    self.gradW = np.zeros_like(self.W)\n",
        "    self.gradb = np.zeros_like(self.b)\n",
        "\n",
        "  def update_output(self, input):\n",
        "    self.output = np.dot(input * self.W.T) + self.b\n",
        "    return self.output\n",
        "\n",
        "  def updateGradInput(self, input, gradOutput):\n",
        "    self.gradInput = np.dot(gradOutput * self.W)\n",
        "    retrun self.gradInput\n",
        "\n",
        "  def accGradParameters(self, input, gradOutput):\n",
        "    self.gradW += np.dot(gradOutput.T, input)\n",
        "    self.gradb += np.sum(gradOutput, axixs = 0)\n",
        "\n",
        "  def zeroGradParameters(self):\n",
        "    self.gradW.fill(0)\n",
        "    self.gradb.fill(0)\n",
        "\n",
        "  def getParameters(self):\n",
        "    return [self.W, self.b]\n",
        "\n",
        "  def getGradParameters(self):\n",
        "    return [self.gradW, self.gradb]\n",
        "\n",
        "  def __repr__(self):\n",
        "    s = self.W.shape\n",
        "    q = 'Linear %d -> %d' %(s[1],s[0])\n",
        "    return q"
      ],
      "metadata": {
        "id": "xWFAXrsUtzmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftMax(Module):\n",
        "  def __init__(self):\n",
        "    super(SoftMax, self).__init__()\n",
        "\n",
        "  def update_output(self,input):\n",
        "    self.output = np.subtract(input, input.max(axis = 1, keepdims = True))\n",
        "    self.output = np.exp(self.output)\n",
        "    self.output = self.output / np.sum(self.output, axis = 1, keepdims = True)\n",
        "\n",
        "    return self.output\n",
        "\n",
        "  def updateGradInput(self, input, gradOutput):\n",
        "    dot_product = np.sum(gradOutput * self.output, axis = 1, keepdims = True)\n",
        "    self.gradInput = self.output * dot_product - gradOutput * self.output\n",
        "    return self.gradInput\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"SoftMax\"\n",
        "\n",
        "class LogSoftMax(Module):\n",
        "  def __init__(self):\n",
        "    super(LogSoftMax, self).__init__()\n",
        "\n",
        "  def update_output(self, input):\n",
        "    self.output = np.subtract(input, input.max(axis = 1, keepdims =True))\n",
        "    exp_exit = np.exp(self.output)\n",
        "    sum_exp = np.sum(exp_exit, axis = 1, keepdims = True)\n",
        "    log_sum = np.log(sum_exp)\n",
        "    self.output = self.output - log_sum\n",
        "    return self.output\n",
        "\n",
        "  def updateGradInput(self, input, gradOutput):\n",
        "    softmax_output = np.exp(self.output)\n",
        "    sum_grad = np.sum(gradOutput, axis = 1, keepdims = True)\n",
        "    self.gradInput = gradOutput - softmax_output * sum_grad\n",
        "    return self.gradInput\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"LogSoftMax\"\n",
        "\n",
        "class BatchNormalization(Module):\n",
        "  EPS = 1e-3\n",
        "  def __init__(self, alpha = 0.):\n",
        "    super(BatchNormalization, self).__init__()\n",
        "    self.alpha = alpha\n",
        "    self.moving_mean = None\n",
        "    self.moving_variance = None\n",
        "\n",
        "  def updateOutput(self, input):\n",
        "    if self.training:\n",
        "      self.batch_mean = np.mean(input, axis = 0)\n",
        "      self.batch_var = np.var(input, axis = 0)\n",
        "      self.std = np.sqrt(self.batch_var + self.EPS)\n",
        "      self.normalized = (input - self.batch_mean) / self.std\n",
        "\n",
        "      if self.moving_mean == None:\n",
        "        self.moving_mean = self.batch_mean\n",
        "        self.moving_variance = self.self.batch_var\n",
        "\n",
        "      else:\n",
        "        self.moving_mean = self.moving_mean * self.alpha + self.batch_mean * (1 - self.alpha)\n",
        "        self.moving_variance = self.moving_variance * self.alpha + self.batch_variance * (1 - self.alpha)\n",
        "\n",
        "      self.output = self.normalized\n",
        "    else:\n",
        "      self.output = (input - self.moving_mean) / np.sqrt(self.moving_variance + self.EPS)\n",
        "    return self.output\n",
        "\n",
        "  def updateGradInput(self, input, gradOutput):\n",
        "        if self.training:\n",
        "            m = input.shape[0]\n",
        "            dL_dy = gradOutput\n",
        "            grad_input = dL_dy / self.std\n",
        "            grad_input -= np.mean(dL_dy, axis=0) / self.std\n",
        "            grad_input -= self.normalized * np.mean(dL_dy * self.normalized, axis=0) / self.std\n",
        "\n",
        "            self.gradInput = grad_input\n",
        "        else:\n",
        "            self.gradInput = gradOutput / np.sqrt(self.moving_variance + self.EPS)\n",
        "\n",
        "        return self.gradInput\n",
        "\n",
        "  def __repr__(self):\n",
        "        return \"BatchNormalization\""
      ],
      "metadata": {
        "id": "49UCduAYHq8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelwiseScaling(Module):\n",
        "    \"\"\"\n",
        "       Implements linear transform of input y = \\gamma * x + \\beta\n",
        "       where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n",
        "    \"\"\"\n",
        "    def __init__(self, n_out):\n",
        "        super(ChannelwiseScaling, self).__init__()\n",
        "\n",
        "        stdv = 1./np.sqrt(n_out)\n",
        "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "\n",
        "        self.gradGamma = np.zeros_like(self.gamma)\n",
        "        self.gradBeta = np.zeros_like(self.beta)\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = input * self.gamma + self.beta\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = gradOutput * self.gamma\n",
        "        return self.gradInput\n",
        "\n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        self.gradBeta = np.sum(gradOutput, axis=0)\n",
        "        self.gradGamma = np.sum(gradOutput*input, axis=0)\n",
        "\n",
        "    def zeroGradParameters(self):\n",
        "        self.gradGamma.fill(0)\n",
        "        self.gradBeta.fill(0)\n",
        "\n",
        "    def getParameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "    def getGradParameters(self):\n",
        "        return [self.gradGamma, self.gradBeta]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ChannelwiseScaling\""
      ],
      "metadata": {
        "id": "bH9hfZk5l5i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dropout(Module):\n",
        "  def __init__(self):\n",
        "    super(Dropout, self).__init__()\n",
        "    self.p = p\n",
        "    self.mask = None\n",
        "\n",
        "  def updateOutput(self, input):\n",
        "    if self.trainig:\n",
        "      self.mask = np.random.binomial(1, 1 - self.p, size = input.shape)\n",
        "      self.output = (input * self.mask) / (1 - self.p)\n",
        "    else:\n",
        "      self.output = input\n",
        "\n",
        "    return self.output\n",
        "\n",
        "  def updateGradInput(self, input, gradOutput):\n",
        "    if self.trainig:\n",
        "      self.gradInput = gradOutput * self.mask / (1 - self.p)\n",
        "    else:\n",
        "      self.gradInput = gradOutput\n",
        "\n",
        "    return self.gradInput\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"Dropout\"\n",
        "\n",
        "class ReLU(Module):\n",
        "    def __init__(self):\n",
        "         super(ReLU, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input):\n",
        "        self.output = np.maximum(input, 0)\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, gradOutput):\n",
        "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ReLU\"\n",
        "\n",
        "class LeakyReLu(Module):\n",
        "  def __init__(self, slope = 0.03):\n",
        "    super(LeakyReLu, self).__init__()\n",
        "    self.slope = slope\n",
        "\n",
        "  def updateOutput(self, input):\n",
        "    self.output = np.where(input >= 0, input, self.slope * input)\n",
        "    return self.output\n",
        "\n",
        "  def updateGradInput(self, input, gradOutput):\n",
        "    self.gradInput = gradOutput * np.where(input > 0, 1, self.slope)\n",
        "    return self.gradInput\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"LeakyReLu\"\n",
        "\n",
        "class ELU(Module):\n",
        "  def __init__(self, alpha = 1.0):\n",
        "    super(ELU, self).__init__()\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def updateOutput(self, input):\n",
        "    self.output = np.where(input > 0, input, self.alpha * (np.exp ** input - 1))\n",
        "    return self.output\n",
        "\n",
        "  def updateGradInput(self, input, gradOutput):\n",
        "    self.gradInput = gradOutput * np.where(input > 0, 1, self.alpha * np.exp ** input)\n",
        "    return self.gradInput\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"ELU\"\n",
        "\n",
        "class SoftPlus(Module):\n",
        "  def __init__(self):\n",
        "    super(SoftPlus, self).__init__()\n",
        "\n",
        "  def updateOutput(self, input):\n",
        "    self.output = np.log1p(np.exp(input))\n",
        "    return self.output\n",
        "\n",
        "  def updateGradInput(self, input, gradOutput):\n",
        "    self.gradInput = gradOutput * (np.exp(input) / (1 + np.exp(input)))\n",
        "    return self.gradInput\n",
        "\n",
        "class Criterion(object):\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the loss function\n",
        "            associated to the criterion and return the result.\n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateOutput`.\n",
        "        \"\"\"\n",
        "        return self.updateOutput(input, target)\n",
        "\n",
        "    def backward(self, input, target):\n",
        "        \"\"\"\n",
        "            Given an input and a target, compute the gradients of the loss function\n",
        "            associated to the criterion and return the result.\n",
        "\n",
        "            For consistency this function should not be overrided,\n",
        "            all the code goes in `updateGradInput`.\n",
        "        \"\"\"\n",
        "        return self.updateGradInput(input, target)\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        \"\"\"\n",
        "        Function to override.\n",
        "        \"\"\"\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want\n",
        "        to have readable description.\n",
        "        \"\"\"\n",
        "        return \"Criterion\"\n",
        "\n",
        "class MSECriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        super(MSECriterion, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MSECriterion\"\n",
        "\n",
        "class ClassNLLCriterionUnstable(Criterion):\n",
        "    EPS = 1e-15\n",
        "    def __init__(self):\n",
        "        a = super(ClassNLLCriterionUnstable, self).__init__()\n",
        "        super(ClassNLLCriterionUnstable, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "        log_probs = np.log(input_clamp)\n",
        "        loss = -np.sum(log_probs * target) / input.shape[0]\n",
        "        self.output = loss\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "        self.gradInput = -target / (input_clamp * input.shape[0])\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterionUnstable\"\n",
        "\n",
        "class ClassNLLCriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        a = super(ClassNLLCriterion, self).__init__()\n",
        "        super(ClassNLLCriterion, self).__init__()\n",
        "\n",
        "    def updateOutput(self, input, target):\n",
        "        self.output = -np.sum(input * target) / input.shape[0]\n",
        "        return self.output\n",
        "\n",
        "    def updateGradInput(self, input, target):\n",
        "        self.gradInput = -target / input.shape[0]\n",
        "        return self.gradInput\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"ClassNLLCriterion\"\n",
        "\n",
        "def sgd_momentum(variables, gradients, config, state):\n",
        "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
        "    state.setdefault('accumulated_grads', {})\n",
        "\n",
        "    var_index = 0\n",
        "    for current_layer_vars, current_layer_grads in zip(variables, gradients):\n",
        "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
        "\n",
        "            old_grad = state['accumulated_grads'].setdefault(var_index, np.zeros_like(current_grad))\n",
        "\n",
        "            np.add(config['momentum'] * old_grad, config['learning_rate'] * current_grad, out=old_grad)\n",
        "\n",
        "            current_var -= old_grad\n",
        "            var_index += 1\n",
        "\n",
        "def adam_optimizer(variables, gradients, config, state):\n",
        "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
        "    state.setdefault('m', {})  # first moment vars\n",
        "    state.setdefault('v', {})  # second moment vars\n",
        "    state.setdefault('t', 0)   # timestamp\n",
        "    state['t'] += 1\n",
        "    for k in ['learning_rate', 'beta1', 'beta2', 'epsilon']:\n",
        "        assert k in config, config.keys()\n",
        "\n",
        "    var_index = 0\n",
        "    lr_t = config['learning_rate'] * np.sqrt(1 - config['beta2']**state['t']) / (1 - config['beta1']**state['t'])\n",
        "    for current_layer_vars, current_layer_grads in zip(variables, gradients):\n",
        "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
        "            var_first_moment = state['m'].setdefault(var_index, np.zeros_like(current_grad))\n",
        "            var_second_moment = state['v'].setdefault(var_index, np.zeros_like(current_grad))\n",
        "            np.add(var_first_moment * config['beta1'], (1 - config['beta1']) * current_grad, out=var_first_moment)\n",
        "            np.add(var_second_moment * config['beta2'], (1 - config['beta2']) * current_grad * current_grad, out=var_second_moment)\n",
        "            current_var -= lr_t * var_first_moment / (np.sqrt(var_second_moment) + config['epsilon'])\n",
        "            assert var_first_moment is state['m'].get(var_index)\n",
        "            assert var_second_moment is state['v'].get(var_index)\n",
        "            var_index += 1\n",
        "\n"
      ],
      "metadata": {
        "id": "e4Crhihtn0Sx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNndNoUAZErYyvFxkrBFZ5b"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}